# Interpreting-human-emotional-responses-from-real-time-video-capture
The developed interface is part of the COMPANION-CM project [1], which aims to create more social assistive robots. One of the keys to achieving this goal is for these machines to be able to interpret and act in response to human responses generated by the robot's own actions. And the fact is that every human reaction involves an emotion, which is the focus of this work. 

For our problem, it is not necessary to differentiate such specific emotions, but rather it is sufficient to differentiate three types of categories: positive, negative and neutral. What we are really interested in is whether the action that the robot has decided to perform is well received or not by the user. For example, what does it mean to express happiness as a response to an action, under our criteria, we understand and classify the emotion happiness as a positive response. Following this criterion, we relabelled the AffectNet database and created an additional one in order to adjust the model to the user it is currently assisting.
