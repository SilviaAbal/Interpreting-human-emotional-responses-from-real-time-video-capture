# Interpreting human emotional responses from real-time video capture
The developed interface is part of the COMPANION-CM project [1], which aims to create more social assistive robots. One of the keys to achieving this goal is for these machines to be able to interpret and act in response to human responses generated by the robot's own actions. 

Convolutional Neural Networks (CNN) are used to carry out this part of the robot's AI, in order to extract features from video recordings. As for the training of the CNN, it is carried out with relevant training data, such as the AffectNet database [8], achieving a robust feature extraction system. AffectNet contains about 1 million facial images collected from the Internet, making it one of the largest databases of facial expressions in existence. In it, up to eleven categorical labels of emotions and non-emotions can be distinguished (Neutral, Happiness, Sadness, Surprise, Fear, Disgust, Anger, Contempt, None, Uncertainty, Faceless).

For our problem, it is not necessary to differentiate such specific emotions, but rather it is sufficient to differentiate three types of categories: positive, negative and neutral. What we are really interested in is whether the action that the robot has decided to perform is well received or not by the user. For example, what does it mean to express happiness as a response to an action, under our criteria, we understand and classify the emotion happiness as a positive response. Following this criterion, we relabelled the AffectNet database and created an additional one in order to adjust the model to the user it is currently assisting. In this way, the pre-trained model is finely tuned to the expressions of the actual patient and his or her environment, making it more effective in the context of application. 

<p align="center">
<img src = "images/tabla_eng.JPG" width="700" />
</p>

# Table of contents

# Dependencies
To install the required packages, run ``pip install -r requirements.txt``
# Project Structure


    Interpreting-human-emotional-responses-from-real-time-video-capture/                               
    ├── AffectNet_relabelled
    │  ├── positive
    │  ├── negative
    │  └── neutral
    ├── 3-fold
    │   ├── 1fold
    │   │   ├── train
    │   │   │  ├── positive
    │   │   │  ├── negative
    │   │   │  └── neutral
    │   │   └──  test
    │   │   │  ├── positive
    │   │   │  ├── negative
    │   │   │  └── neutral
    │   ├── 2fold
    │   │   ├── train
    │   │   │  ├── positive
    │   │   │  ├── negative
    │   │   │  └── neutral
    │   │   └──  test
    │   │   │  ├── positive
    │   │   │  ├── negative
    │   │   │  └── neutral
    │   └──  3fold
    │   │   ├── train
    │   │   │  ├── positive
    │   │   │  ├── negative
    │   │   │  └── neutral
    │   │   └──  test
    │   │   │  ├── positive
    │   │   │  ├── negative
    │   │   │  └── neutral
    ├── classifier
    │   ├── haarcascade_eye
    │   └── haarcascade_frontalface_default
    ├── pretrained_models
    │   ├── model_display_1.pt
    │   ├── model_display_2.pt
    │   ├── model_display_3.pt
    │   └── model_finetunning.pt
    ├── weak_loss_layer
    │   └── weak_loss.py
    ├── config.py
    ├── dataset_creation.py
    ├── emotions.py
    ├── evaluation.py
    ├── LICENSE
    └── README.md

# Usage
To be able to use this emotion detection system, you must first download this repository and install the dependencies if necessary.

``
git clone https://github.com/atulapra/Emotion-detection.git
``

The implemented code allows to re-train the model if desired or to use the system in real time using a webcam with a pre-trained model.

The idea of fine-tuning the model is to make it work better with the people the robot is assisting, hence the creation of its own database. To be able to re-train it, our database can be used with permission together with the affecnet database to avoid possible overfitting. In the same way, if you have your own database, you can use it, but you have to take into account the file hierarchy and file syntax.

To train the model:

``
python emotions.py --mode train
``

To detect human reactions in real time:

``
python emotions.py --mode display
``

# Datasets
- Own Dataset
https://drive.google.com/drive/folders/187Pg1hq5Bi1o-dYWYC47xSVxOLf8Pyte?usp=sharing
- AffectNet Dataset

# Data preparation

    .
    ├── build                   # Compiled files (alternatively `dist`)
    ├── docs                    # Documentation files (alternatively `doc`)
    ├── src                     # Source files (alternatively `lib` or `app`)
    ├── test                    # Automated tests (alternatively `spec` or `tests`)
    ├── tools                   # Tools and utilities
    ├── LICENSE
    └── README.md
